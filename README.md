# Heritage Health Project Guide
The below guide outlines the process flow for this project. A combination of SPSS Modeler Streams, R files, Jupyter notebooks, and Excel were used to handle the data. It is importantly to note that unless otherwise stated data in both the train and test datasets were handled in two separate, but analogous, files (i.e. the same processing was done). 

1. *Stream1.str* used to join relational tables into Train (Claims Y1 + DIH Y2) and Test (Claims Y2 + DIH Y3) datasets. Output files were called DIH_Y1_Target and DIH_Y2_Target respectively; they have not been uploaded to GitHub due to their large file size.
2. *Agg_Y1.R* and *Agg_Y2.R* were used to clean the Train and Test datasets respectively before aggregation.
3. Output files from R Studio were called *Dummy1.csv* and *Dummy2.csv* and were brought into Python using Jupyter notebooks for aggregation. The Jupyter notebooks are called *DIH_Y1_dummy_dtypeFix.ipynb* and *DIH_Y2_dummy_dtypeFix.ipynb*. Python has an easier interface for aggregating tables.
4. Output files from Jupyter notebook were called *out_Agg_dummy_Y1.csv* and *out_Agg_dummy_Y2.csv* and were brought into Excel for processing of field names. Python introduced unncessary unicode characters into the column title text which would have not been handled properly in R Studio in modeling. The files were resaved as csv's and brought back into R Studio.
5. Once in R studio, a little bit more pre-processing was done before modeling. Rest of major analysis was conducted in R.
6. *Stream2.str* indicates some side analysis done within SPSS modeler using association/market basket analysis of categorical data (i.e. on the *Dummy1.csv* and *Dummy2.csv* files) as well as feature selection on the dummy variable dataset (i.e. *out_Agg_dummy_Y1.csv* and *out_Agg_dummy_Y2.csv*) to try and reduce the size of predictor variables.
